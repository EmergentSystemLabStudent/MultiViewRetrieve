<!DOCTYPE html>
<html>
<head lang="en">
  <meta name="keywords" content="MultiViewRetrieve">
  <title>Object Instance Retrieval in Assistive Robotics: Leveraging Fine-Tuned SimSiam with Multi-View Images Based on 3D Semantic Map</title>
  <style>
  .container {
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100px;
    flex-direction: column;
    text-align: center;
  }

    .fixed-width {
      width: 700px;
      margin: 10px auto;
    }

    .center-align {
      text-align: center;
    }

    .text-justify {
      text-align: justify;
    }

    video {
      width: 700px;
      height: auto;
      display: block;
      margin: 10px auto;
    }

    .col-md-8 {
      width: 700px;
      margin: 10px auto;
    }

    .github-link {
      display: flex;
      justify-content: center;
    }

    .github-link a {
      margin: 10px;
    }

    .image-container {
      display: flex;
      justify-content: center;
    }

    .image-container img {
      width: 700px;
      height: auto;
    }

    .video-container {
        margin: 10px;
        display: flex;
        justify-content: center;
    }
    
    .video-container iframe {
        width: 700px;
        height: 394px;
    }
  </style>
</head>
<body>
  <div class="container" id="main">
    <div class="row">
      <h2 class="col-md-12">
        Object Instance Retrieval in Assistive Robotics:<br>Leveraging Fine-Tuned SimSiam with Multi-View Images<br>Based on 3D Semantic Map
      </h2>
    </div>
  </div>
  
  <p class="center-align">Taichi Sakaguchi, <a href="https://scholar.google.com/citations?hl=en&user=jtB7J0AAAAAJ" target=“_blank” rel=“noopener noreferrer”>Akira Taniguchi</a>, <a href="https://scholar.google.com/citations?hl=en&user=Y4qjYvMAAAAJ" target=“_blank” rel=“noopener noreferrer”>Yoshinobu Hagiwara</a>, <a href="https://scholar.google.co.jp/citations?user=tsm7qaQAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Lotfi El Hafi</a>, <a href="https://scholar.google.co.jp/citations?user=KPxSCJUAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Shoichi Hasegawa</a>, <a href="https://scholar.google.com/citations?hl=en&user=dPOCLQEAAAAJ" target=“_blank” rel=“noopener noreferrer”>Tadahiro Taniguchi</a></p>
  
  <div class="github-link">
    <a href="https://github.com/EmergentSystemLabStudent/MultiViewRetrieve" target=“_blank” rel=“noopener noreferrer”>Github</a>
    <!-- <a href="" target=“_blank” rel=“noopener noreferrer”>Paper</a>
    <a href="" target=“_blank” rel=“noopener noreferrer”>Slide</a> -->
  </div>

  <div class="video-container">
      <iframe src="https://www.youtube.com/embed/vv9k_wPkfIs?si=8zLy3EsD6SoPXLBy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </div>
  
  <!-- <div class="video-container">
      <iframe src="https://www.youtube.com/embed/n8se-MgPi50?si=B5B60-8XXMO-j2CC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </div> -->
  
  <h2 class="fixed-width">Abstract</h2>
  <p class="center-align fixed-width text-justify">
    Robots that assist in daily life are required to locate specific instances of objects that match the user's desired object in the environment.
    This task is known as Instance-Specific Image Goal Navigation (InstanceImageNav), which requires a model capable of distinguishing between different instances within the same class. 
    One significant challenge in robotics is that when a robot observes the same object from various 3D viewpoints, its appearance may differ greatly, making it difficult to recognize and locate the object accurately.
    In this study, we introduce a method, <b>SimView</b>, that leverages multi-view images based on a 3D semantic map of the environment and self-supervised learning by SimSiam to train an instance identification model on-site. 
    The effectiveness of our approach is validated using a photorealistic simulator, Habitat Matterport 3D, created by scanning real home environments.
    Our results demonstrate a 1.7-fold improvement in task accuracy compared to CLIP, which is pre-trained multimodal contrastive learning for object search.
    This improvement highlights the benefits of our proposed fine-tuning method in enhancing the performance of assistive robots in InstanceImageNav tasks.  </p>


  <h2 class="fixed-width">Approach</h2>
  <p class="center-align fixed-width text-justify">
    In the proposed system, a robot explores the environment, identifies the instance identical to a given query image from among the collected object images, and uses a 3D semantic map of the environment to locate the target object's position. 
    In addition, we propose a method, <b>Semantic Instance Multi-view Contrastive Fine-tuning (SimView)</b>, for fine-tuning pre-trained models using a self-supervised learning framework to improve task accuracy in the environment.
    Figure 1 shows the diagram of our proposed system.  </p>
  
  <div class="image-container">
    <img src="./images/habitat_approach.svg">
  </div>

  <h2 class="fixed-width">Registration Module</h2>
  <p class="center-align fixed-width text-justify">
    We set up exploration points at the same intervals in the free space on a 2D map, and the robot explores the environment by visiting these exploration points. 
    While the robot explores the environment, 2D mask images are generated using ray-tracing with the segmented 3D map and camera pose.
    Generating mask images from the 3D map allows the same object to be associated across different frames. 
    The observed mask images are converted into Bounding Boxes (BBoxes), and the robot extracts the BBoxes regions from the observed RGB images to observe the objects. 
    When extracting the BBoxes of each object from the RGB image, it is adjusted to match the longer side of the BBox. 
    In addition, areas outside the original RGB image are interpolated with only black.
    The images of observed objects are pre-processed and fed into a pre-trained encoder to convert them into feature vectors.
    Additionally, when the same object is observed multiple times in the environment, the observed feature vectors for each object are recorded as a single set.  </p>

  <h2 class="fixed-width">Self-Supervised Fine-tuning Module</h2>
  <p class="center-align fixed-width text-justify">
    This module fine-tuned the image encoder, which was pre-trained by contrastive learning using self-supervised learning, object images observed by the robot while exploring the environment, and their pseudo-labels.
    When a robot explores the environment and observes objects, images of the same instance include images observed from various angles of view.
    In a preliminary experiment, we confirmed that when fine-tuning a pre-trained model using only contrastive learning on such a dataset, the accuracy of discrimination between instances is worse than that of the trained model.
    Therefore, we propose a method to train a linear classifier simultaneously with contrastive learning. 
    We use object instance ID \( y_true \) obtained from a 3D semantic map of the robot's environment as pseudo labels.
    In addition, the contrastive learning method using negative pairs is recommended to be trained with a very large batch size and requires a large amount of data for learning.
    Then, to conduct fine-tuning, it is necessary to continue exploring the environment for a long time and collecting images of objects.
    Therefore, we use SimSiam for fine-tuning, which allows learning even with a small batch size.  </p>

  <h2 class="fixed-width">Retrieval Module</h2>
  <p class="center-align fixed-width text-justify">
  The given query image \(I_q\) is input into a pre-trained encoder, resulting in a feature vector denoted as \(\bm{q}\).
  Furthermore, we represent the observed feature vectors with instance ID \(i\) as a set \(\bm{Z}_i = \{\bm{z}_{i,n}\}_{n=1}^{N_i}\).
  The cosine similarity between the query image \(\bm{q}\) and observed feature vectors \(\bm{Z}_i\) is calculated as \(\text{CosSim}(\bm{z}_{i, n}, \bm{q}) = \frac{\bm{z}_{i, n} \cdot \bm{q}}{\|\bm{z}_{i, n}\| \|\bm{q}\|}\) for each set of feature vectors corresponding to each instance ID.
  From multiple similarities, the maximum value \(m_i\) is selected.
  Finally, the instance ID \(J_{\text{target}}\) with the highest similarity among the set of maximum similarities \(\{m_{j}\}_{j=1}^J\) for each instance is obtained.
  The target object's position is obtained using the instance ID \(J_{\text{target}}\) from the search results and the 3D semantic map.
  The robot could navigate to the target position using the map.</p>

<div class="col-md-8">
  <h2 class="text-center">Citation</h2>
  <p class="text-justify">
    This paper is under review on IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2024), 
    and the BibTeX for the paper is below.
    <textarea id="bibtex" class="form-control" readonly rows="6" cols="105">
      @inproceedings{domainbridgingnav2024,
        title={Object Instance Retrieval in Assistive Robotics: Leveraging Fine-Tuned SimSiam with Multi-View Images Based on 3D Semantic Map},
        author={Taichi Sakaguchi and Akira Taniguchi and Yoshinobu Hagiwara and Lotfi El Hafi and Shoichi Hasegawa and Tadahiro Taniguchi},
        booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
        year={2024 UnderReview}
}</textarea>
  </p>
</div>

  <div class="col-md-8">
    <h2 class="text-center">Other links</h2>
    <p class="text-justify">
      <ul>
        <li><a href="http://www.em.ci.ritsumei.ac.jp/" target=“_blank” rel=“noopener noreferrer”>Laboratory website</a></li>
        <li><a href="" target=“_blank” rel=“noopener noreferrer”>Demo video of this research</a></li>
        <li><a href="https://www.youtube.com/watch?v=UBgZGRG00eA" target=“_blank” rel=“noopener noreferrer”>Demo video of related research</a></li>
      </ul>
    </p>
  </div>

  <div class="col-md-8">
    <h2 class="text-center">Acknowledgements</h2>
    <p class="text-justify">
      This work was supported by JSPS KAKENHI Grants-in-Aid for Scientific Research (Grant Numbers JP23K16975, 22K12212) JST Moonshot Research & Development Program (Grant Number JPMJMS2011).
    </p>
  </div>
  
  <script src="script.js"></script>
</body>
</html>
